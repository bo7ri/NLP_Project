@ARTICLE{8664495,
  author={Kane, Thomas B.},
  journal={IEEE Technology and Society Magazine}, 
  title={Artificial Intelligence in Politics: Establishing Ethics}, 
  year={2019},
  volume={38},
  number={1},
  pages={72-80},
  doi={10.1109/MTS.2019.2894474}
}
@article{ANSARI20201821,
title = {Analysis of Political Sentiment Orientations on Twitter},
journal = {Procedia Computer Science},
volume = {167},
pages = {1821-1828},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306669},
author = {Mohd Zeeshan Ansari and M.B. Aziz and M.O. Siddiqui and H. Mehra and K.P. Singh},
keywords = {Political Orientations, Opinion Mining, Text Classification, Twitter},
abstract = {The dramatic increase in the number of users on social media platform leads to the generation of huge amount of unstructured text in the form of messages, chats, posts and blogs. Besides the exchange of information, social media is a remarkably convenient medium to express the ideas and opinions which gain popularity when liked by a large set of users. This popularity may reflect the sentiment of people towards that person, organization or a place. The social media platform, such as Twitter, generates huge amounts of the text containing political insights, which can be mined to analyze the people’s opinion and predict the future trends in the elections. In this work, an attempt is made to the mine tweets, capture the political sentiments from it and model it as a supervised learning problem. The extraction of tweets pertaining to the General Elections of India in 2019 is carried out along with the study of sentiments among Twitter users towards the major national political parties participating in the electoral process. Subsequently, the classification model based on sentiments is prepared to predict the inclination of tweets to infer the results of the elections. The Long Short Term Memory (LSTM) is employed to prepare the classification model and compare it with the classical machine learning models.}
}
@misc{sentiment140,
  title={Sentiment140 Dataset},
  url={https://www.tensorflow.org/datasets/catalog/sentiment140},
year={2010},
  note={Accessed: 2023-12-10}
}
@misc{uselection2020tweets,
  title={US Election 2020 Tweets Dataset},
  author={Man Chun Hui},
  url={https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets},
  year={2021},
  note={Accessed: 2023-12-10}
}

@misc{inan2023llamaguard,
  title={Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
  howpublished={GenAI at Meta},
  year={2023},
  month={12},
  note={Available: \url{https://github.com/facebookresearch/repo}; Blogpost: \url{https://ai.meta.com/blog/?page=1}},
  abstract={We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model’s capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
  email={Correspondence: inan@meta.com},
  url={https://github.com/facebookresearch/repo},
  blogpost={https://ai.meta.com/blog/?page=1}
}

@phdthesis{ahmed2021nlp,
  title = {Natural Language Processing Techniques for Political Opinion \& Sentiment},
  author = {Ahmed, Nabib},
  year = {2022},
  school = {Harvard College},
  advisor = {Sweeney, Latanya},
  type = {Bachelor's thesis},
  accessdate = {2022-03-08},
  issued = {2022-02-24},
  submitted = {2022},
  other = {28864438},
  url = {https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370953},
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@book{keselj2020nlp,
  author    = {V. Keselj},
  title     = {Introduction to Probabilistic NLP Notes},
  year      = {2020},
  howpublished = {CSCI 4152/6509 - Natural Language Processing, Faculty of Computer Science, Dalhousie University, Halifax, NS}
}

@article{vaswani2017attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention is All You Need},
  journal = {arXiv},
  year = {2017},
  url = {https://arxiv.org/abs/1706.03762}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Terechshenko2020,
    author = {Zhanna Terechshenko and Fridolin Linder and Vishakh Padmakumar and Fengyuan Liu and Jonathan Nagler and Joshua A. Tucker and Richard Bonneau},
    title = {A Comparison of Methods in Political Science Text Classification: Transfer Learning Language Models for Politics},
    journal = {SSRN Electronic Journal},
    year = {2020},
    doi = {10.2139/ssrn.3724644},
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{Political1,
author = {Elghazaly, Tarek and Mahmoud, Amal and Hefny, Hesham A.},
title = {Political Sentiment Analysis Using Twitter Data},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896396},
doi = {10.1145/2896387.2896396},
abstract = {There is a remarkable growth in the usage of social networks, such as Facebook and Twitter. Users from different cultures and backgrounds post large volumes of textual comments reflecting their opinion in different aspect of life and make them available to everyone. In particular we study the case of Twitter and focus on presidential elections in Egypt 2012. This paper compares between two techniques for Arabic text classification using WEKA application. These techniques are Support Vector Machine (SVM) and Na\"{\i}ve Bayesian (NB), we investigate the use of TF-IDF to obtain document vector. The main objective of this paper is to measure the accuracy and time to get the result for each classifier and to determine which classifier is more accurate for Arabic text classification.Comparison reported in this paper shows that the Na\"{\i}ve Bayesian method is the highest accuracy and the lowest error rate.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {11},
numpages = {5},
keywords = {WEKA, SVM, Arabic Language, Na\"{\i}ve Bayes, Social Network, Twitter, Sentiment Analysis},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{Kharde2016SentimentAO,
  title={Sentiment Analysis of Twitter Data : A Survey of Techniques},
  author={Vishal. A. Kharde and Sheetal S. Sonawane},
  journal={ArXiv},
  year={2016},
  volume={abs/1601.06971},
  url={https://api.semanticscholar.org/CorpusID:15847221}
}

